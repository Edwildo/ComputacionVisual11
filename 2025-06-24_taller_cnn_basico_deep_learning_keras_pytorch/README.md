# Taller - Redes Convolucionales desde Cero: Reconocimiento de ImÃ¡genes con PyTorch

## ğŸ§  Objetivo
Construir, entrenar y evaluar un modelo de red neuronal convolucional (CNN) desde cero usando PyTorch, aplicÃ¡ndolo sobre el dataset MNIST para clasificaciÃ³n de dÃ­gitos escritos a mano.

---

## ğŸ“¦ Dataset utilizado
**MNIST** â€“ Contiene 60,000 imÃ¡genes de entrenamiento y 10,000 de prueba, en escala de grises (28x28 pÃ­xeles), con etiquetas del 0 al 9.

---
A continuaciÃ³n se muestra una muestra aleatoria de 10 imÃ¡genes del set de entrenamiento:
![Muestra MNIST](pytorch/mnist_sample_batch.png)

## ğŸ§± Arquitectura de la Red CNN

Conv2D (1 â†’ 32 filtros, kernel 3x3) â†’ ReLU â†’ MaxPooling (2x2)
â†’ Conv2D (32 â†’ 64 filtros, kernel 3x3) â†’ ReLU â†’ MaxPooling (2x2)
â†’ Flatten â†’ Fully Connected (128 neuronas) â†’ ReLU â†’ Output (10 clases)


## âš™ï¸ Entrenamiento del modelo

- **FunciÃ³n de pÃ©rdida:** `CrossEntropyLoss`
- **Optimizador:** `Adam`
- **Ã‰pocas:** *5*
- **Batch size:** 64

### ğŸ“ˆ Curvas de entrenamiento

*(pytorch/training_curves.png)*

---

## ğŸ§ª EvaluaciÃ³n del modelo

-NÃºmero de Ã©pocas: 5

-PrecisiÃ³n final (train): â‰ˆ 98%

-Tiempo de entrenamiento: unos pocos segundos por Ã©poca en CPU.




## ReflexiÃ³n

Durante este taller aprendÃ­ cÃ³mo las capas convolucionales extraen caracterÃ­sticas espaciales en imÃ¡genes y cÃ³mo su combinaciÃ³n con funciones de activaciÃ³n y pooling mejora el desempeÃ±o del modelo.

Cambios como aumentar filtros o aÃ±adir Dropout pueden aumentar precisiÃ³n o reducir sobreajuste.